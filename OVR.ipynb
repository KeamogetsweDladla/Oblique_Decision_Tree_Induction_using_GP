{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33c60188-2d69-467b-ad08-d960291e3db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ucimlrepo in c:\\users\\keamodladla\\anaconda3\\lib\\site-packages (0.0.7)\n",
      "Requirement already satisfied: pandas>=1.0.0 in c:\\users\\keamodladla\\anaconda3\\lib\\site-packages (from ucimlrepo) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in c:\\users\\keamodladla\\anaconda3\\lib\\site-packages (from ucimlrepo) (2024.6.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\keamodladla\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\keamodladla\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\keamodladla\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\keamodladla\\anaconda3\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\keamodladla\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Install the UIC repo\n",
    "! pip install ucimlrepo\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedShuffleSplit, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from gplearn.genetic import SymbolicClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import time\n",
    "from statistics import mean, stdev\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "875c568b-18bb-440e-9010-61d476f000f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the databases\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch datasets\n",
    "iris = fetch_ucirepo(id=53) \n",
    "wine_quality = fetch_ucirepo(id=186) \n",
    "rice_cammeo_and_osmancik = fetch_ucirepo(id=545) \n",
    "breast_cancer_wisconsin_original = fetch_ucirepo(id=15) \n",
    "magic_gamma_telescope = fetch_ucirepo(id=159) \n",
    "banknote_authentication = fetch_ucirepo(id=267)\n",
    "yeast = fetch_ucirepo(id=110)\n",
    "letter_recognition = fetch_ucirepo(id=59) \n",
    "\n",
    "datasets = [iris, wine_quality,rice_cammeo_and_osmancik,breast_cancer_wisconsin_original,\n",
    "            magic_gamma_telescope, banknote_authentication, yeast, letter_recognition]\n",
    "\n",
    "datasets_strings = ['iris', 'wine_quality','rice_cammeo_and_osmancik','breast_cancer_wisconsin_original',\n",
    "            'magic_gamma_telescope', 'banknote_authentication', 'yeast', 'letter_recognition']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3304a126-1bf8-4c80-911d-49e649e4f926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe \n",
    "col_names = ['database', 'mean_score', 'mean_std','mean_size', 'size_std', 'ave_time','std_time']  \n",
    "results_df  = pd.DataFrame(columns = col_names) \n",
    "results_df['database'] = datasets_strings\n",
    "\n",
    "# Iterate over datasets\n",
    "acc_scores = []\n",
    "std_scores = []\n",
    "depth_scores = []\n",
    "depth_std_scores = []\n",
    "ave_time_scores = []\n",
    "std_time_scores = []\n",
    "\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # Preprocess dataset, and split into training and test sets\n",
    "    # data as pandas dataframes \n",
    "    X = ds.data.features \n",
    "    y = ds.data.targets \n",
    "\n",
    "    # Drop empty entries, if any\n",
    "    empty_row_idx = [index for index, row in X.iterrows() if row.isnull().any()]\n",
    "    if len(empty_row_idx) >= 1:\n",
    "        X = X.drop(index=empty_row_idx)\n",
    "        y = y.drop(index=empty_row_idx)\n",
    "        \n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    sss = StratifiedShuffleSplit(n_splits=10, test_size=0.3)    \n",
    "\n",
    "    acc_each_run = []\n",
    "    std_each_run = []\n",
    "    depth_each_run = []\n",
    "    time_each_run = []\n",
    "\n",
    "    \n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Initialise the SymbolicClassifier\n",
    "            function_set = ['add', 'sub', 'mul']\n",
    "            base_cl = SymbolicClassifier(population_size = 100, generations = 50, stopping_criteria = 0.01, function_set=function_set,\n",
    "            p_crossover = 0.7,parsimony_coefficient=0.0005,p_subtree_mutation = 0.1, p_hoist_mutation = 0.05, p_point_mutation = 0.1,\n",
    "            max_samples = 0.9, verbose = 0)\n",
    "\n",
    "            # Wrap the base classifier with OneVsRestClassifier\n",
    "            ovr_classifier = OneVsRestClassifier(base_cl)\n",
    "            \n",
    "            # Train the classifier\n",
    "            ovr_classifier.fit(X_train,y_train)\n",
    "\n",
    "            end_time = time.time()\n",
    "        \n",
    "            score = cross_val_score(ovr_classifier, X_test, y_test, cv=10)\n",
    "            exec_duration = end_time-start_time\n",
    "            #tree_depth = ovr_classifier.tree_.max_depth\n",
    "\n",
    "            acc_each_run.append(score.mean().round(3))\n",
    "            std_each_run.append(score.std().round(3))\n",
    "            depth_each_run.append(1)\n",
    "            time_each_run.append(exec_duration)\n",
    "\n",
    "    ave_score = mean(acc_each_run)\n",
    "    ave_std = mean(std_each_run)\n",
    "    ave_depth = mean(depth_each_run)\n",
    "    ave_depth_std = stdev(depth_each_run)\n",
    "    ave_time = mean(time_each_run)\n",
    "    time_std = stdev(time_each_run)\n",
    "\n",
    "    acc_scores.append(ave_score)\n",
    "    std_scores.append(ave_std)\n",
    "    depth_scores.append(ave_depth)\n",
    "    depth_std_scores.append(ave_depth_std)\n",
    "    ave_time_scores.append(ave_time)\n",
    "    std_time_scores.append(time_std)\n",
    "\n",
    "results_df[ 'mean_score'] = acc_scores\n",
    "results_df[ 'mean_std'] = std_scores\n",
    "results_df[ 'mean_size'] = depth_scores\n",
    "results_df[ 'size_std'] = [round(x,3) for x in depth_std_scores]\n",
    "results_df[ 'ave_time'] = [round(x,3) for x in ave_time_scores]\n",
    "results_df[ 'std_time'] = [round(x,3) for x in std_time_scores]\n",
    "results_df.set_index(\"database\", inplace = True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a4e31c-9984-467d-b924-2cc175a19f72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
